{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''00_firstModel.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a model whose loss function is categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model whose loss function is categorical_crossentropy'\n",
    "''' For categorical_crossentropy '''\n",
    "model_ce = Sequential()\n",
    "model_ce.add(Dense(128, input_dim=200))\n",
    "model_ce.add(Activation('sigmoid'))\n",
    "model_ce.add(Dense(256))\n",
    "model_ce.add(Activation('sigmoid'))\n",
    "model_ce.add(Dense(5))\n",
    "model_ce.add(Activation('softmax'))\n",
    "\n",
    "''' Set up the optimizer '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "sgd = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)\n",
    "\n",
    "''' Compile model with specified loss and optimizer '''\n",
    "model_ce.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=sgd,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 30\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_ce = model_ce.fit(X_train, Y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          nb_epoch=nb_epoch,\n",
    "                          verbose=0,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.1)\n",
    "\n",
    "'''Access the loss and accuracy in every epoch'''\n",
    "loss_ce\t= history_ce.history.get('loss')\n",
    "acc_ce \t= history_ce.history.get('acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_ce)), loss_ce,label='CE')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_ce)), acc_ce,label='CE')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "# plt.savefig('00_firstModel.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 00_lossFuncSelection.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''01_lossFuncSelection.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model whose loss function is categorical_crossentropy'\n",
    "''' For categorical_crossentropy '''\n",
    "model_ce = Sequential()\n",
    "model_ce.add(Dense(128, input_dim=200))\n",
    "model_ce.add(Activation('sigmoid'))\n",
    "model_ce.add(Dense(256))\n",
    "model_ce.add(Activation('sigmoid'))\n",
    "model_ce.add(Dense(5))\n",
    "model_ce.add(Activation('softmax'))\n",
    "\n",
    "print 'Building a model whose loss function is mean_squared_error'\n",
    "''' For mean_squared_error '''\n",
    "model_mse = Sequential()\n",
    "model_mse.add(Dense(128, input_dim=200))\n",
    "model_mse.add(Activation('sigmoid'))\n",
    "model_mse.add(Dense(256))\n",
    "model_mse.add(Activation('sigmoid'))\n",
    "model_mse.add(Dense(5))\n",
    "model_mse.add(Activation('softmax'))\n",
    "\n",
    "''' Set up the optimizer '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "sgd = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)\n",
    "\n",
    "''' Compile model with specified loss and optimizer '''\n",
    "model_ce.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=sgd,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_mse.compile(loss= 'mean_squared_error',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 30\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_ce = model_ce.fit(X_train, Y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          nb_epoch=nb_epoch,\n",
    "                          verbose=0,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.1)\n",
    "\n",
    "history_mse = model_mse.fit(X_train, Y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            verbose=0,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "'''Access the loss and accuracy in every epoch'''\n",
    "loss_ce\t= history_ce.history.get('loss')\n",
    "acc_ce \t= history_ce.history.get('acc')\n",
    "loss_mse= history_mse.history.get('loss')\n",
    "acc_mse = history_mse.history.get('acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_ce)), loss_ce,label='CE')\n",
    "plt.plot(range(len(loss_mse)), loss_mse,label='MSE')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_ce)), acc_ce,label='CE')\n",
    "plt.plot(range(len(acc_mse)), acc_mse,label='MSE')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('01_lossFuncSelection.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 01_lossFuncSelection.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''02_lossFuncSelection.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 30\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building model using SGD(lr=0.1)'\n",
    "''' 1. Model using large learning rate '''\n",
    "model_large = Sequential()\n",
    "model_large.add(Dense(128, input_dim=200))\n",
    "model_large.add(Activation('sigmoid'))\n",
    "model_large.add(Dense(256))\n",
    "model_large.add(Activation('sigmoid'))\n",
    "model_large.add(Dense(5))\n",
    "model_large.add(Activation('softmax'))\n",
    "\n",
    "''' set the learning rate of SGD optimizer to 0.1 '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "sgd010 = SGD(lr=0.1,momentum=0.0,decay=0.0,nesterov=False)\n",
    "\n",
    "model_large.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer=sgd010,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_large = model_large.fit(X_train, Y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                nb_epoch=nb_epoch,\n",
    "                                verbose=0,\n",
    "                                shuffle=True,\n",
    "                                validation_split=0.1)\n",
    "\n",
    "loss_large = history_large.history.get('loss')\n",
    "acc_large = history_large.history.get('acc')\n",
    "\n",
    "print 'Building model using SGD(lr=0.01)'\n",
    "''' 2. Model using median learning rate '''\n",
    "model_median = Sequential()\n",
    "model_median.add(Dense(128, input_dim=200))\n",
    "model_median.add(Activation('sigmoid'))\n",
    "model_median.add(Dense(256))\n",
    "model_median.add(Activation('sigmoid'))\n",
    "model_median.add(Dense(5))\n",
    "model_median.add(Activation('softmax'))\n",
    "\n",
    "''' set the learning rate of SGD optimizer to 0.01 '''\n",
    "sgd001 = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)\n",
    "\n",
    "model_median.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer=sgd001,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_median = model_median.fit(X_train, Y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  nb_epoch=nb_epoch,\n",
    "                                  verbose=0,\n",
    "                                  shuffle=True,\n",
    "                                  validation_split=0.1)\n",
    "\n",
    "loss_median = history_median.history.get('loss')\n",
    "acc_median = history_median.history.get('acc')\n",
    "\n",
    "print 'Building model using SGD(lr=0.001)'\n",
    "''' 3. Model using small learning rate '''\n",
    "model_small = Sequential()\n",
    "model_small.add(Dense(128, input_dim=200))\n",
    "model_small.add(Activation('sigmoid'))\n",
    "model_small.add(Dense(256))\n",
    "model_small.add(Activation('sigmoid'))\n",
    "model_small.add(Dense(5))\n",
    "model_small.add(Activation('softmax'))\n",
    "\n",
    "''' set the learning rate of SGD optimizer to 0.001 '''\n",
    "sgd0001 = SGD(lr=0.001,momentum=0.0,decay=0.0,nesterov=False)\n",
    "\n",
    "model_small.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer=sgd0001,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_small = model_small.fit(X_train, Y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                nb_epoch=nb_epoch,\n",
    "                                verbose=0,\n",
    "                                shuffle=True,\n",
    "                                validation_split=0.1)\n",
    "\n",
    "loss_small = history_small.history.get('loss')\n",
    "acc_small = history_small.history.get('acc')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_large)), loss_large,label='lr=0.1')\n",
    "plt.plot(range(len(loss_median)), loss_median,label='lr=0.01')\n",
    "plt.plot(range(len(loss_small)), loss_small,label='lr=0.001')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_large)), acc_large,label='lr=0.1')\n",
    "plt.plot(range(len(acc_median)), acc_median,label='lr=0.01')\n",
    "plt.plot(range(len(acc_small)), acc_small,label='lr=0.001')\n",
    "plt.title('Accuracy')\n",
    "plt.savefig('02_learningRateSelection.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 02_learningRateSelection.png'\n",
    "\n",
    "loss_bm = loss_median\n",
    "acc_bm = acc_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''03_activationFunc.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 30\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building model using softplus as activation function'\n",
    "''' Use softplus as our activation function '''\n",
    "model_sp = Sequential()\n",
    "model_sp.add(Dense(128, input_dim=200))\n",
    "model_sp.add(Activation('softplus'))\n",
    "model_sp.add(Dense(256))\n",
    "model_sp.add(Activation('softplus'))\n",
    "model_sp.add(Dense(5))\n",
    "model_sp.add(Activation('softmax'))\n",
    "\n",
    "''' Use SGD(lr=0.01) as the optimizer  '''\n",
    "''' lr set to 0.01 according to 02_learningRateSelection.py '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "sgd = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)\n",
    "\n",
    "model_sp.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_sp = model_sp.fit(X_train, Y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          nb_epoch=nb_epoch,\n",
    "                          verbose=0,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.1)\n",
    "\n",
    "loss_sp = history_sp.history.get('loss')\n",
    "acc_sp = history_sp.history.get('acc')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(2)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_sp)),loss_sp,label='Softplus')\n",
    "plt.plot(range(len(loss_bm)),loss_bm,label='Sigmoid')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_sp)),acc_sp,label='Softplus')\n",
    "plt.plot(range(len(acc_bm)),acc_bm,label='Sigmoid')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('03_activationFuncSelection.png',dpi=300,format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''04_optimizerSelection.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 30\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model whose optimizer=adam, activation function=softplus'\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(128, input_dim=200))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(256))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(5))\n",
    "model_adam.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as Adam '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "model_adam.compile(loss= 'categorical_crossentropy',\n",
    "              \t\toptimizer='Adam',\n",
    "              \t\tmetrics=['accuracy'])\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_adam = model_adam.fit(X_train, Y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              nb_epoch=nb_epoch,\n",
    "                              verbose=0,\n",
    "                              shuffle=True,\n",
    "                              validation_split=0.1)\n",
    "\n",
    "loss_adam= history_adam.history.get('loss')\n",
    "acc_adam = history_adam.history.get('acc')\n",
    "\n",
    "print 'Building a model whose optimizer=sgd, activation function=softplus'\n",
    "model_sgd = Sequential()\n",
    "model_sgd.add(Dense(128, input_dim=200))\n",
    "model_sgd.add(Activation('softplus'))\n",
    "model_sgd.add(Dense(256))\n",
    "model_sgd.add(Activation('softplus'))\n",
    "model_sgd.add(Dense(5))\n",
    "model_sgd.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as SGD '''\n",
    "model_sgd.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='SGD',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history_sgd = model_sgd.fit(X_train, Y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            verbose=0,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "loss_sgd= history_sgd.history.get('loss')\n",
    "acc_sgd = history_sgd.history.get('acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(4)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_adam)), loss_adam,label='Adam')\n",
    "plt.plot(range(len(loss_sgd)), loss_sgd,label='SGD')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_adam)), acc_adam,label='Adam')\n",
    "plt.plot(range(len(acc_sgd)), acc_sgd,label='SGD')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('04_optimizerSelection.png',dpi=300,format='png')\n",
    "\n",
    "#print 'Result saved into 04_optimizerSelection.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''05_overfittingCheck.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 50\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model whose optimizer=adam, activation function=softplus'\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(128, input_dim=200))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(256))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(5))\n",
    "model_adam.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as Adam '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "model_adam.compile(loss= 'categorical_crossentropy',\n",
    "                   optimizer='Adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_adam = model_adam.fit(X_train, Y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              nb_epoch=nb_epoch,\n",
    "                              verbose=0,\n",
    "                              shuffle=True,\n",
    "                              validation_split=0.1)\n",
    "\n",
    "loss_adam= history_adam.history.get('loss')\n",
    "acc_adam = history_adam.history.get('acc')\n",
    "\n",
    "''' Access the performance on validation data '''\n",
    "val_loss_adam = history_adam.history.get('val_loss')\n",
    "val_acc_adam = history_adam.history.get('val_acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(4)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_adam)), loss_adam,label='Training')\n",
    "plt.plot(range(len(val_loss_adam)), val_loss_adam,label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_adam)), acc_adam,label='Training')\n",
    "plt.plot(range(len(val_acc_adam)), val_acc_adam,label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('05_overfittingCheck.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 05_overfittingCheck.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''06_regularizer.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' Import l1,l2 (regularizer) '''\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 50\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model without regularizer L2'\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(128, input_dim=200))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(256))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(5))\n",
    "model_adam.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as Adam '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "model_adam.compile(loss= 'categorical_crossentropy',\n",
    "                   optimizer='Adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_adam = model_adam.fit(X_train, Y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              nb_epoch=nb_epoch,\n",
    "                              verbose=0,\n",
    "                              shuffle=True,\n",
    "                              validation_split=0.1)\n",
    "\n",
    "''' Access the performance on validation data '''\n",
    "loss_adam = history_adam.history.get('loss')\n",
    "val_loss_adam = history_adam.history.get('val_loss')\n",
    "val_acc_adam = history_adam.history.get('val_acc')\n",
    "\n",
    "print 'Building a model with regularizer L2'\n",
    "model_l2 = Sequential()\n",
    "model_l2.add(Dense(128, input_dim=200, W_regularizer=l2(0.01)))\n",
    "model_l2.add(Activation('softplus'))\n",
    "model_l2.add(Dense(256, W_regularizer=l2(0.01)))\n",
    "model_l2.add(Activation('softplus'))\n",
    "model_l2.add(Dense(5, W_regularizer=l2(0.01)))\n",
    "model_l2.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as Adam '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "model_l2.compile(loss= 'categorical_crossentropy',\n",
    "              \toptimizer='Adam',\n",
    "              \tmetrics=['accuracy'])\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_l2 = model_l2.fit(X_train, Y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          nb_epoch=nb_epoch,\n",
    "                          verbose=0,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.1)\n",
    "\n",
    "loss_l2 = history_l2.history.get('loss')\n",
    "val_loss_l2 = history_l2.history.get('val_loss')\n",
    "val_acc_l2 = history_l2.history.get('val_acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(2)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_adam)), loss_adam,label='Training')\n",
    "plt.plot(range(len(val_loss_adam)), val_loss_adam,label='Validation')\n",
    "plt.title('Original')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(loss_l2)), loss_l2,label='Training')\n",
    "plt.plot(range(len(val_loss_l2)), val_loss_l2,label='Validation')\n",
    "plt.title('With Regularizer')\n",
    "#plt.show()\n",
    "plt.savefig('06_regularizer.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 06_regularizer.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''07_earlystopping.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' EarlyStopping '''\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 50\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model whose optimizer=adam, activation function=softplus'\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(128, input_dim=200))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(256))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dense(5))\n",
    "model_adam.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as Adam '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "model_adam.compile(loss= 'categorical_crossentropy',\n",
    "                   optimizer='Adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_adam = model_adam.fit(X_train, Y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              nb_epoch=nb_epoch,\n",
    "                              verbose=0,\n",
    "                              shuffle=True,\n",
    "                              validation_split=0.1,\n",
    "                              callbacks=[early_stopping])\n",
    "\n",
    "loss_adam = history_adam.history.get('loss')\n",
    "acc_adam = history_adam.history.get('acc')\n",
    "val_loss_adam = history_adam.history.get('val_loss')\n",
    "val_acc_adam = history_adam.history.get('val_acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(5)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_adam)), loss_adam,label='Training')\n",
    "plt.plot(range(len(val_loss_adam)), val_loss_adam,label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_adam)), acc_adam,label='Training')\n",
    "plt.plot(range(len(val_acc_adam)), val_acc_adam,label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('07_earlystopping.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 07_earlystopping.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''08_dropout.py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import theano and numpy '''\n",
    "import theano\n",
    "import numpy as np\n",
    "execfile('00_readingInput.py')\n",
    "\n",
    "''' set the size of mini-batch and number of epochs'''\n",
    "batch_size = 16\n",
    "nb_epoch = 100\n",
    "\n",
    "''' Import keras to build a DL model '''\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "print 'Building a model with dropout = 0.4'\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(128, input_dim=200))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dropout(0.4))\n",
    "model_adam.add(Dense(256))\n",
    "model_adam.add(Activation('softplus'))\n",
    "model_adam.add(Dropout(0.4))\n",
    "model_adam.add(Dense(5))\n",
    "model_adam.add(Activation('softmax'))\n",
    "\n",
    "''' Setting optimizer as Adam '''\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad\n",
    "model_adam.compile(loss= 'categorical_crossentropy',\n",
    "                   optimizer='Adam',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "'''Fit models and use validation_split=0.1 '''\n",
    "history_adam = model_adam.fit(X_train, Y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              nb_epoch=nb_epoch,\n",
    "                              verbose=0,\n",
    "                              shuffle=True,\n",
    "                              validation_split=0.1)\n",
    "\n",
    "loss_adam= history_adam.history.get('loss')\n",
    "acc_adam = history_adam.history.get('acc')\n",
    "val_loss_adam = history_adam.history.get('val_loss')\n",
    "val_acc_adam = history_adam.history.get('val_acc')\n",
    "\n",
    "''' Visualize the loss and accuracy of both models'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(8)\n",
    "plt.subplot(121)\n",
    "plt.plot(range(len(loss_adam)), loss_adam,label='Training')\n",
    "plt.plot(range(len(val_loss_adam)), val_loss_adam,label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(acc_adam)), acc_adam,label='Training')\n",
    "plt.plot(range(len(val_acc_adam)), val_acc_adam,label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('08_dropout.png',dpi=300,format='png')\n",
    "\n",
    "print 'Result saved into 08_dropout.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
